{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bongjoonsiong/Pipeline_input_tf.feature_col/blob/main/Tutorial_titanic_problem_tf_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI7knaZK8jnd"
      },
      "source": [
        "# Train tf.keras model using feature coulmns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yet5I-fme3p"
      },
      "source": [
        "In this tutorial, we will see how to use tf.keras model to classify structured data (pandas dataframe)  with creating an input pipe line using feature columns ( tf.feature_column) and tf.data.\n",
        "\n",
        "you will learn-\n",
        "\n",
        "\n",
        "* Creating different types of feature columns using tf.feature_columns\n",
        "* Creating input data function using tf.data for train, val and test set\n",
        "* Creating, compiling and training of tf.keras.model\n",
        "* Evaluating model\n",
        "* Prediction on test data\n",
        "\n",
        "## The Dataset\n",
        "\n",
        "I have used [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/overview) from kaggle, you can [download](https://www.kaggle.com/c/3136/download-all) and find [description](https://www.kaggle.com/c/titanic/data) of dataset on kaggle. I have used google colab and hence uploaded data in google drive.\n",
        "\n",
        "## Mount google drive\n",
        "I have uploaded data on **google drive,** Learn How to use data from google drive [here](https://medium.com/ml-book/simplest-way-to-open-files-from-google-drive-in-google-colab-fae14810674)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BspuOkXyIoWw"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs0qZTKg-yc1"
      },
      "source": [
        "# Import TensorFlow and other libraries\n",
        "I have used Tensorflow nightly version which is unstable version (aug 2019)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP1u_-4EIsMB"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  !pip install tf-nightly-gpu-2.0-preview\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "\n",
        "\n",
        "!pip install sklearn\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import feature_column\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "print(\"GPU Available: \", tf.test.is_gpu_available())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfCp-DEn_Yw9"
      },
      "source": [
        "# Load and preprocess Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5d73uRWLuvf"
      },
      "source": [
        "## Use Pandas to create a dataframe\n",
        "\n",
        "[Pandas](https://pandas.pydata.org/) is a Python library with many helpful utilities for loading and working with structured data. We will use Pandas to download the dataset from mounted google drive, and load it into a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7_n4-mbJG5I"
      },
      "source": [
        "data = pd.read_csv('drive/My Drive/collab data/titanic/train.csv')\n",
        "data.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfJwsS34JXQC"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wFRWzcI7Ala"
      },
      "source": [
        "data['Survived'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRlUaDDq7QAk"
      },
      "source": [
        "data_minority = data[data['Survived']==1]\n",
        "data_minority.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGCJMEBg7rdQ"
      },
      "source": [
        "data_majority = data[data['Survived']==0][:342]\n",
        "data_majority.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGIgCGX77yay"
      },
      "source": [
        "new_df = data_minority.sample(549, replace = True)\n",
        "new_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41zujmuN8zAH"
      },
      "source": [
        "normal_distributed_df = pd.concat([data_majority, new_df], axis = 0)\n",
        "normal_distributed_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5le5wHD93--"
      },
      "source": [
        "normal_distributed_df['Survived'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X8nutS9-uU2"
      },
      "source": [
        "from sklearn.utils import resample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swaXOBAl-wjD"
      },
      "source": [
        "normal_distributed_df = pd.concat([data_majority, data_minority], axis = 0)\n",
        "normal_distributed_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6bD9zki_1mz"
      },
      "source": [
        "# Shuffle dataframe rows\n",
        "new_df = resample(normal_distributed_df, replace = True, n_samples = 5000, random_state=42)\n",
        "\n",
        "new_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnLX8Z0FAJhz"
      },
      "source": [
        "normal_distributed_df['Survived'].value_counts(normalize = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp8M7MgH__tq"
      },
      "source": [
        "new_df['Survived'].value_counts(normalize = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znOOxh0KMFdw"
      },
      "source": [
        "## Missing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw70CXSIMOId"
      },
      "source": [
        "### Check missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgIfUfC2Kbtf"
      },
      "source": [
        "data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApZ8aEfHMmbk"
      },
      "source": [
        "### Missing value handling\n",
        "\n",
        "As you can seee that there are some missing values in 'age' , 'embark' and 'cabin'. In 'cabin' number of missing values are large hence we delete this column from data, and in 'age' we will fill missing values with mean value and in 'embark' with most frequent value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IZp5IeSLhj9"
      },
      "source": [
        "mean_value = round(data['Age'].mean())\n",
        "mode_value = data['Embarked'].mode()[0]\n",
        "\n",
        "value = {'Age': mean_value, 'Embarked': mode_value}\n",
        "data.fillna(value=value,inplace=True)\n",
        "\n",
        "data.dropna(axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYbAH5QzNIaQ"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBLt7eYBTg8p"
      },
      "source": [
        "## Explore data with pandas_profiling library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipRMZsdSJ27S"
      },
      "source": [
        "import pandas_profiling as pdpf\n",
        "pdpf.ProfileReport(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UDmGsYH_vb1"
      },
      "source": [
        "# Train, val, test Split\n",
        "\n",
        "We will divide data into train, validation, test data with 3:1:1 ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cpfPx9kSI5K"
      },
      "source": [
        "train, test = train_test_split(data, test_size=0.2)\n",
        "train, val = train_test_split(train, test_size=0.25)\n",
        "print(len(train), 'train examples')\n",
        "print(len(val), 'validation examples')\n",
        "print(len(test), 'test examples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "908_JL0P_4OV"
      },
      "source": [
        "# Input pilpe line\n",
        "\n",
        "## Create an input pipeline using tf.data\n",
        "\n",
        "Next, we will wrap the dataframes with [tf.data](https://www.tensorflow.org/guide/datasets). This will enable us  to use feature columns as a bridge to map from the columns in the Pandas dataframe to features used to train the model. If we were working with a very large CSV file (so large that it does not fit into memory), we would use tf.data to read it from disk directly. That is not covered in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aobXv0rWSVsE"
      },
      "source": [
        "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('Survived')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc-lGRoSSWL_"
      },
      "source": [
        "batch_size = 32\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
        "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34t2mMOQrjBR"
      },
      "source": [
        "## Understand the input pipeline\n",
        "\n",
        "Now that we have created the input pipeline, let's call it to see the format of the data it returns. We have used a small batch size to keep the output readable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYcS3I7USjxC"
      },
      "source": [
        "for feature_batch, label_batch in train_ds.take(1):\n",
        "  print('Every feature:', list(feature_batch.keys()))\n",
        "  print('A batch of ages:', feature_batch['Age'])\n",
        "  print('A batch of targets:', label_batch )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grNDOEKFrlFx"
      },
      "source": [
        " We can see that the dataset returns a dictionary of column names (from the dataframe) that map to column values from rows in the dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpJVpcgUAGSm"
      },
      "source": [
        "## Feature columns\n",
        "\n",
        "Know more about feature columns [here](https://medium.com/ml-book/demonstration-of-tensorflow-feature-columns-tf-feature-column-3bfcca4ca5c4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XcRchQtAGHH"
      },
      "source": [
        "### Decide which types of features you have in data\n",
        "While data exploration you should note the types of features we have, for example, whether a feature is numerical or categorical, if it is numerical then can we categorize it into buckets or not, or if it is categorical then it should be checked how many categories are there, can we convert it into indicator columns or embedding column, are there any two feature, those can we combined to create new crossed feature. I will recommend you to read this very simplified [tutorial on feature columns](https://medium.com/ml-book/demonstration-of-tensorflow-feature-columns-tf-feature-column-3bfcca4ca5c4)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qBXyJIhKu2F"
      },
      "source": [
        "#numarical features\n",
        "num_c = ['Age','Fare','Parch','SibSp']\n",
        "bucket_c  = ['Age'] #bucketized numerical feature\n",
        "\n",
        "#categorical features\n",
        "cat_i_c = ['Embarked', 'Pclass','Sex'] #indicator columns\n",
        "cat_e_c = ['Ticket'] # embedding column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XarZtFjd6xGh"
      },
      "source": [
        "### Scaler function\n",
        "It is very important for numerical variables to get scaled. here I have used min-max scaling. Here we are creating a function named 'get_scal' which takes list of numerical features and  returns 'minmax' function, which will be used in tf.feature_column.numeric_column() as normalizer_fn in parameters. 'minmax' function itself takes a 'numerical' number from a particular feature and return scaled value of that number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA-RZuUkh3mu"
      },
      "source": [
        "def get_scal(feature):\n",
        "  def minmax(x):\n",
        "    mini = train[feature].min()\n",
        "    maxi = train[feature].max()\n",
        "    return (x - mini)/(maxi-mini)\n",
        "  return(minmax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca-6KKB79FN_"
      },
      "source": [
        "### Creating feature columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adAWWyoI9N7H"
      },
      "source": [
        "#### Numerical Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoUEEJ7WSkS5"
      },
      "source": [
        "feature_columns = []\n",
        "for header in num_c:\n",
        "  scal_input_fn = get_scal(header)\n",
        "  feature_columns.append(feature_column.numeric_column(header, normalizer_fn=scal_input_fn))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8rQzLha9lEn"
      },
      "source": [
        "#### Bucketized columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBCSVONpTE-G"
      },
      "source": [
        "Age = feature_column.numeric_column(\"Age\")\n",
        "# bucketized cols\n",
        "age_buckets = feature_column.bucketized_column(Age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
        "feature_columns.append(age_buckets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO_pu1b3-SCs"
      },
      "source": [
        "#### Categorical Indicator columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6CfmwpQUYKm"
      },
      "source": [
        "for feature_name in cat_i_c:\n",
        "  vocabulary = data[feature_name].unique()\n",
        "  cat_c = tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary)\n",
        "  one_hot = feature_column.indicator_column(cat_c)\n",
        "  feature_columns.append(one_hot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9LOMwJz-cqn"
      },
      "source": [
        "#### Categorical Embedding columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU-KE8xjVSgr"
      },
      "source": [
        "for feature_name in cat_e_c:\n",
        "  vocabulary = data[feature_name].unique()\n",
        "  cat_c = tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary)\n",
        "  embeding = feature_column.embedding_column(cat_c, dimension=50)\n",
        "  feature_columns.append(embeding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMq_THxZ-oxX"
      },
      "source": [
        "#### Crosed columns\n",
        "Combination of 'age' (age buckets) and 'sex'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTcCauWXdgde"
      },
      "source": [
        "vocabulary = data['Sex'].unique()\n",
        "Sex = tf.feature_column.categorical_column_with_vocabulary_list('Sex', vocabulary)\n",
        "\n",
        "crossed_feature = feature_column.crossed_column([age_buckets, Sex], hash_bucket_size=1000)\n",
        "crossed_feature = feature_column.indicator_column(crossed_feature)\n",
        "feature_columns.append(crossed_feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y32eYPVkb_0J"
      },
      "source": [
        "print('Total number of feature coumns: ',len(feature_columns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfbmPf6OAOCb"
      },
      "source": [
        "# Create, compile and train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmRsU9iI_djv"
      },
      "source": [
        "### Create a feature layer\n",
        "Now that we have defined our feature columns, we will use a [DenseFeatures](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/DenseFeatures) layer to input them to our Keras model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9AlMTTpSyj9"
      },
      "source": [
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivC3uG7nAc9R"
      },
      "source": [
        "#### tf.keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leptwzjqXJDa"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  feature_layer,\n",
        "  layers.Dense(16, kernel_regularizer=tf.keras.regularizers.l2(0.01), activation='relu'),\n",
        "  layers.Dense(64, kernel_regularizer=tf.keras.regularizers.l2(0.01), activation='relu'),\n",
        "  layers.Dropout(0.2),\n",
        "\n",
        "  layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_ds,\n",
        "          validation_data=val_ds,\n",
        "          epochs=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nNIlICVAqDP"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF3r3p5Qnv9H"
      },
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP5NfLE3BuB3"
      },
      "source": [
        "## Train vs Val 'accuracy' and 'loss'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_JiEjKDBsiZ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "epochs = range(20)\n",
        "\n",
        "plt.title('Accuracy')\n",
        "plt.plot(epochs,  history.history['accuracy'], color='blue', label='Train')\n",
        "plt.plot(epochs, history.history['val_accuracy'], color='orange', label='Val')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "_ = plt.figure()\n",
        "plt.title('Loss')\n",
        "plt.plot(epochs, history.history['loss'], color='blue', label='Train')\n",
        "plt.plot(epochs, history.history['val_loss'], color='orange', label='Val')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr4jNpju0JnC"
      },
      "source": [
        "we can see out model is ovefiting, to get a good model we have to tune hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPnR3OuIHiVi"
      },
      "source": [
        "# Problem Test Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxyQsDu9HxP_"
      },
      "source": [
        "## Load and preprocess test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_K5RuiTt8D9"
      },
      "source": [
        "test_data = pd.read_csv('drive/My Drive/collab data/titanic/test.csv')\n",
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJZChzrTuPV2"
      },
      "source": [
        "test_data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmkjQfcXuR7C"
      },
      "source": [
        "mean_value = round(data['Age'].mean())\n",
        "mean_value1 = data['Fare'].mean()\n",
        "\n",
        "value = {'Age': mean_value, 'Fare': mean_value1}\n",
        "test_data.fillna(value=value,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Xxpisofv9CZ"
      },
      "source": [
        "test_data.dropna(axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOmagsGrH-ga"
      },
      "source": [
        "## Input function for test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itERkfi0vYnq"
      },
      "source": [
        "def test_input_fn(features, batch_size=256):\n",
        "    \"\"\"An input function for prediction.\"\"\"\n",
        "    # Convert the inputs to a Dataset without labels.\n",
        "    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUlJqxejvhQ5"
      },
      "source": [
        "test_predict = test_input_fn(dict(test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C_FzoyFIHTP"
      },
      "source": [
        "## Prediction\n",
        "Predicting proability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2Gf0xihvhwV"
      },
      "source": [
        "predicted_ar=model.predict(test_predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNDtvE5kISGn"
      },
      "source": [
        "### Prediction DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54RB-K3PwQmf"
      },
      "source": [
        "predict_df = test_data[['PassengerId']]\n",
        "predict_df['Survived'] = predicted_ar\n",
        "predict_df['Survived'] = predict_df['Survived'].apply(lambda x: 1 if x>=.5 else 0) #converting probability into class\n",
        "predict_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgBl4uQIKa1s"
      },
      "source": [
        "# End"
      ]
    }
  ]
}